{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Owner Categorization with an RNN\n",
    "\n",
    "In this notebook, I will implement a recurrent neural network that categorize owners base on their name. Using an RNN rather than a feedfoward network is more accurate since we can include information about the *sequence* of words. Here we'll use a dataset of owners name from the Philipines and India.\n",
    "\n",
    "The architecture for this network is shown below.\n",
    "\n",
    "Here, we'll pass in words to an embedding layer.\n",
    "\n",
    "From the embedding layer, the new representations will be passed to LSTM cells. These will add recurrent connections to the network so we can include information about the sequence of words in the data. Finally, the LSTM cells will go to a sigmoid output layer here. The output layer will just be a single unit then, with a sigmoid activation function.\n",
    "\n",
    "We are not interested in the sigmoid outputs except for the very last one, we can ignore the rest. We'll calculate the cost from the output of the last step and the training label.\n",
    "\n",
    "Charles Jansen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf #TensorFlow 1.0\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xlrd in c:\\programdata\\anaconda3\\lib\\site-packages\n"
     ]
    }
   ],
   "source": [
    "!pip install xlrd\n",
    "fullExcel = pd.read_excel(\"owners.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 181337 entries, 0 to 181336\n",
      "Data columns (total 3 columns):\n",
      "ownername                     181337 non-null object\n",
      "nonPortfolioHolderTypeName    181337 non-null object\n",
      "Country                       181337 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 4.2+ MB\n"
     ]
    }
   ],
   "source": [
    "fullExcel.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ownername</th>\n",
       "      <th>nonPortfolioHolderTypeName</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DBH International Private Limited</td>\n",
       "      <td>Company</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Karan Thapar</td>\n",
       "      <td>Person</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Karun Carpets Private Limited</td>\n",
       "      <td>Company</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lotus Global Investments Ltd</td>\n",
       "      <td>Company</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S. K. Toshniwal</td>\n",
       "      <td>Person</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           ownername nonPortfolioHolderTypeName Country\n",
       "0  DBH International Private Limited                    Company   India\n",
       "1                       Karan Thapar                     Person   India\n",
       "2      Karun Carpets Private Limited                    Company   India\n",
       "3       Lotus Global Investments Ltd                    Company   India\n",
       "4                    S. K. Toshniwal                     Person   India"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullExcel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "names = np.array(fullExcel.ownername)\n",
    "categories =  np.array(fullExcel.nonPortfolioHolderTypeName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DBH International Private Limited' 'Karan Thapar'\n",
      " 'Karun Carpets Private Limited']\n",
      "['Company' 'Person' 'Company']\n"
     ]
    }
   ],
   "source": [
    "print(names[:3])\n",
    "print(categories[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data preprocessing\n",
    "\n",
    "Since we're using embedding layers, we'll need to encode each word with an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "all_names = ' '.join(names)\n",
    "words = all_names.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DBH International Private Limited Karan Thapar Karun Carpets Private Limited Lotus Global Investments Ltd S. K. Toshniwal Vijay D. Rai Bharath Chmpaklal Sutaria Sridhar reddy Gireddy Srinivasa Reddy Arikatla Surender Reddy Bhimavarapu Vasantha Madasu Vijay SanathBai Chokshi Dharmayug Investments ltd Dilip Kumar Lakhi Gurpreet N S Sobti India - Central Government /State Government Innovative Money Matters Pvt. Ltd. Jagdeep Singh Jasmeet Kaur Sethi KKM Enterprises Pvt. Ltd. Navjeet Singh Sobti Parmeet Kaur Rakan Infrastructures Pvt Ltd Swift Buildwell Pvt. Ltd. Vishvdeva Leasing and Investments pvt ltd Anil Pandit Credit Renaissance Development Fund LP Credit Renaissance Fund Limited Garuda Plant Products Limited Hima Sheth Indrani Khanna Mahindra & Mahindra Limited Smita Patel Surjit Uberoi Trenton Investments Company Private Limited Columbia Wanger Asset Management, L.P FMR LLC Nalanda India Fund Limited Warburg Pincus International Partners, L.P Warburg Pincus Netherlands Internationa'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_names[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DBH',\n",
       " 'International',\n",
       " 'Private',\n",
       " 'Limited',\n",
       " 'Karan',\n",
       " 'Thapar',\n",
       " 'Karun',\n",
       " 'Carpets',\n",
       " 'Private',\n",
       " 'Limited',\n",
       " 'Lotus',\n",
       " 'Global',\n",
       " 'Investments',\n",
       " 'Ltd',\n",
       " 'S.',\n",
       " 'K.',\n",
       " 'Toshniwal',\n",
       " 'Vijay',\n",
       " 'D.',\n",
       " 'Rai']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Encoding the words\n",
    "\n",
    "The embedding lookup requires that we pass in integers to our network. The easiest way to do this is to create dictionaries that map the words in the vocabulary to integers. Then we can convert each of our owner names into integers so they can be passed into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii-1 for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "names_ints = []\n",
    "for each in names:\n",
    "    names_ints.append([vocab_to_int[word] for word in each.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Encoding the labels (= the category here)\n",
    "\n",
    "Our labels are \"company\" or \"person\". To use these labels in our network, we need to convert them to 0 and 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "categories = np.array([1 if each == 'Company' else 0 for each in categories])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ..., 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length names: 0\n",
      "Maximum name length: 28\n"
     ]
    }
   ],
   "source": [
    "names_lens = Counter([len(x) for x in names_ints])\n",
    "print(\"Zero-length names: {}\".format(names_lens[0]))\n",
    "print(\"Maximum name length: {}\".format(max(names_lens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For names shorter than 28, we'll pad with 0s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "seq_len = max(names_lens)\n",
    "features = np.zeros((len(names_ints), seq_len), dtype=int)\n",
    "for i, row in enumerate(names_ints):\n",
    "    features[i, -len(row):] = np.array(row)[:seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0, 13552,    86,     6,\n",
       "            2],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,   588,\n",
       "         1297],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,  5857,  8031,     6,\n",
       "            2],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,  3005,    91,    14,\n",
       "            0],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,    26,    46,\n",
       "         1136],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,   103,    73,\n",
       "          611],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,  5523, 71295,\n",
       "         6034],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,   956,  8098,\n",
       "        69250],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,   691,    49,\n",
       "         9500],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,  1995,    49,\n",
       "         5826]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[:10,:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training, Validation, Test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "With our data in nice shape, we'll split it into training, validation, and test sets.\n",
    "\n",
    "10% randomly taken for test.\n",
    "\n",
    "Kfold on the remaining 90% for validation and training.-->canceled. Done, but took much more time for the same 98.9 result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "X\n",
      "Train set: \t\t(146882, 28) \n",
      "Validation set: \t(16321, 28) \n",
      "Test set: \t\t(18134, 28) \n",
      "Y\n",
      "Train set: \t\t(146882,) \n",
      "Validation set: \t(16321,) \n",
      "Test set: \t\t(18134,)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.9\n",
    "train_val_x, test_x, train_val_y, test_y = train_test_split(\n",
    "    features, categories, \n",
    "    train_size = split_frac)\n",
    "\n",
    "#sin Kfold\n",
    "train_x, val_x, train_y, val_y = train_test_split(\n",
    "    train_val_x, train_val_y, \n",
    "    train_size = split_frac)\n",
    "'''\n",
    "#Kfold\n",
    "train_x = []\n",
    "val_x   = []\n",
    "train_y = []\n",
    "val_y   = []\n",
    "train_x =  np.empty([0,max(names_lens)])\n",
    "val_x   =  np.empty([0,max(names_lens)])\n",
    "train_y =  np.empty(0)\n",
    "val_y   =  np.empty(0)\n",
    "\n",
    "kf = KFold(n_splits = 9, shuffle=True)\n",
    "for train_index, val_index in kf.split(train_val_x):\n",
    "    train_temp_x, val_temp_x = train_val_x[train_index], train_val_x[val_index]\n",
    "    train_temp_y, val_temp_y = train_val_y[train_index], train_val_y[val_index]\n",
    "    train_x = np.concatenate((train_x, train_temp_x), axis=0)\n",
    "    val_x   = np.concatenate((val_x, val_temp_x), axis=0)\n",
    "    train_y = np.concatenate((train_y, train_temp_y), axis=0)\n",
    "    val_y = np.concatenate((val_y, val_temp_y), axis=0)\n",
    "\n",
    "'''\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"X\\nTrain set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape),\n",
    "      \"\\nY\\nTrain set: \\t\\t{}\".format(train_y.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_y.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_y.shape),\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the graph\n",
    "\n",
    "Here, we'll build the graph. First up, defining the hyperparameters.\n",
    "\n",
    "* `lstm_size`: Number of units in the hidden layers in the LSTM cells. \n",
    "* `lstm_layers`: Number of LSTM layers in the network. I'd start with 1, then add more if I'm underfitting.\n",
    "* `batch_size`: The number of names to feed the network in one training pass.\n",
    "* `learning_rate`: Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lstm_size = 64\n",
    "lstm_layers = 1\n",
    "batch_size = 1024\n",
    "learning_rate = 0.01\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For the network itself, we'll be passing in our 28 element long names vectors. Each batch will be `batch_size` vectors. We'll also be using dropout on the LSTM layer, so we'll make a placeholder for the keep probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_words = len(vocab_to_int)\n",
    "\n",
    "# Add nodes to the graph\n",
    "with tf.name_scope('inputs'):\n",
    "    inputs_ = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "with tf.name_scope('labels'):\n",
    "    labels_ = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "with tf.name_scope('keep_prob'):    \n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Embedding\n",
    "\n",
    "Now we'll add an embedding layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79782"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ltd',\n",
       " 'Pvt',\n",
       " 'Limited',\n",
       " '&',\n",
       " 'Ltd.',\n",
       " 'LIMITED',\n",
       " 'Private',\n",
       " 'Kumar',\n",
       " 'Shah',\n",
       " 'Pvt.',\n",
       " 'PRIVATE',\n",
       " 'Fund',\n",
       " 'Investment',\n",
       " 'Patel',\n",
       " 'Investments',\n",
       " 'S',\n",
       " 'LTD',\n",
       " 'P',\n",
       " 'K',\n",
       " 'Company']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Size of the embedding vectors (number of units in the embedding layer)\n",
    "embed_size = 300 \n",
    "\n",
    "with tf.name_scope(\"Embedded\"):\n",
    "    embed = tf.contrib.layers.embed_sequence(inputs_, vocab_size=n_words, embed_dim=embed_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### LSTM cell\n",
    "\n",
    "\n",
    "\n",
    "Next, we'll create our LSTM cells to use in the recurrent network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"RNN_cells\"):\n",
    "    # Your basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # Add dropout to the cell\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n",
    "with tf.name_scope(\"RNN_init_state\"):    \n",
    "    # Getting an initial state of all zeros\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### RNN forward pass\n",
    "\n",
    "\n",
    "Now we need to actually run the data through the RNN nodes. \n",
    "\n",
    "Above I created an initial state, `initial_state`, to pass to the RNN. This is the cell state that is passed between the hidden layers in successive time steps. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"RNN_forward\"):\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, embed,\n",
    "                                             initial_state=initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output\n",
    "\n",
    "We want the final output. So we need to grab the last output with `outputs[:, -1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('predictions'):\n",
    "    predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, activation_fn=tf.sigmoid)\n",
    "    tf.summary.histogram('predictions', predictions)\n",
    "with tf.name_scope('cost'):\n",
    "    cost = tf.losses.mean_squared_error(labels_, predictions)\n",
    "    tf.summary.scalar('cost', cost)\n",
    "with tf.name_scope('train'):    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Validation accuracy\n",
    "\n",
    "Here we can add a few nodes to calculate the accuracy which we'll use in the validation pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy'): \n",
    "    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Batching\n",
    "\n",
    "This is a simple function for returning batches from our data. First it removes data such that we only have full batches. Then it iterates through the `x` and `y` arrays and returns slices out of those arrays with size `[batch_size]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size=100):\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "import os\n",
    "epochs = 1\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        for ii, (x, y) in enumerate(get_batches(train_x, train_y, 2), 1):\n",
    "            print(ii)\n",
    "            print(x)\n",
    "            print(y)\n",
    "            print(x.shape)\n",
    "            print(y.shape)\n",
    "            print(y[:, None])\n",
    "            if ii == 5:\n",
    "                raise Exception(\"Manual Stop\")\n",
    "#''' ;           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "epochs = 3   \n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/1 Iteration: 5 Train loss: 0.152\n",
      "Epoch: 0/1 Iteration: 10 Train loss: 0.027\n",
      "Epoch: 0/1 Iteration: 15 Train loss: 0.022\n",
      "Epoch: 0/1 Iteration: 20 Train loss: 0.015\n",
      "Epoch: 0/1 Iteration: 25 Train loss: 0.020\n",
      "Epoch: 0/1 Iteration: 30 Train loss: 0.013\n",
      "Epoch: 0/1 Iteration: 35 Train loss: 0.023\n",
      "Epoch: 0/1 Iteration: 40 Train loss: 0.016\n",
      "Epoch: 0/1 Iteration: 45 Train loss: 0.011\n",
      "Epoch: 0/1 Iteration: 50 Train loss: 0.012\n",
      "Epoch: 0/1 Iteration: 55 Train loss: 0.012\n",
      "Epoch: 0/1 Iteration: 60 Train loss: 0.006\n",
      "Epoch: 0/1 Iteration: 65 Train loss: 0.012\n",
      "Epoch: 0/1 Iteration: 70 Train loss: 0.007\n",
      "Epoch: 0/1 Iteration: 75 Train loss: 0.006\n",
      "Epoch: 0/1 Iteration: 80 Train loss: 0.008\n",
      "Epoch: 0/1 Iteration: 85 Train loss: 0.005\n",
      "Epoch: 0/1 Iteration: 90 Train loss: 0.009\n",
      "Epoch: 0/1 Iteration: 95 Train loss: 0.009\n",
      "Epoch: 0/1 Iteration: 100 Train loss: 0.011\n",
      "Epoch: 0/1 Iteration: 105 Train loss: 0.007\n",
      "Epoch: 0/1 Iteration: 110 Train loss: 0.006\n",
      "Epoch: 0/1 Iteration: 115 Train loss: 0.011\n",
      "Epoch: 0/1 Iteration: 120 Train loss: 0.008\n",
      "Epoch: 0/1 Iteration: 125 Train loss: 0.008\n",
      "Epoch: 0/1 Iteration: 130 Train loss: 0.009\n",
      "Epoch: 0/1 Iteration: 135 Train loss: 0.005\n",
      "Epoch: 0/1 Iteration: 140 Train loss: 0.009\n",
      "Epoch: 0/3 Iteration: 5 Train loss: 0.126\n",
      "Epoch: 0/3 Iteration: 10 Train loss: 0.017\n",
      "Epoch: 0/3 Iteration: 15 Train loss: 0.022\n",
      "Epoch: 0/3 Iteration: 20 Train loss: 0.013\n",
      "Epoch: 0/3 Iteration: 25 Train loss: 0.019\n",
      "Epoch: 0/3 Iteration: 30 Train loss: 0.014\n",
      "Epoch: 0/3 Iteration: 35 Train loss: 0.023\n",
      "Epoch: 0/3 Iteration: 40 Train loss: 0.015\n"
     ]
    }
   ],
   "source": [
    "for lstm_size in [64,128,256,512]:\n",
    "    for num_layers in [1, 2, 3, 5]:\n",
    "        for learning_rate in [0.001, 0.003, 0.01, 0.03, 0.1]:\n",
    "            for epochs in [1,3,5,10]:\n",
    "                log_string = 'logs/4/lr={},rl={},ru={},e={}'.format(learning_rate, num_layers, lstm_size, epochs)\n",
    "                writer = tf.summary.FileWriter(log_string)\n",
    "                with tf.Session() as sess:\n",
    "                    saver = tf.train.Saver()\n",
    "                    sess.run(tf.global_variables_initializer())\n",
    "                    #file_writer = tf.summary.FileWriter('./logs/3', sess.graph)\n",
    "                    #train_writer = tf.summary.FileWriter('./logs/2/train', sess.graph)\n",
    "                    #test_writer = tf.summary.FileWriter('./logs/2/test')\n",
    "\n",
    "                    iteration = 1\n",
    "                    for e in range(epochs):\n",
    "                        state = sess.run(initial_state)\n",
    "\n",
    "                        for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "                            feed = {inputs_: x,\n",
    "                                    labels_: y[:, None],\n",
    "                                    keep_prob: 0.5,\n",
    "                                    initial_state: state}\n",
    "                            summary, loss, state, _ = sess.run([merged, cost, final_state, optimizer], feed_dict=feed)\n",
    "\n",
    "                            #train_writer.add_summary(summary, iteration)\n",
    "                            writer.add_summary(summary, iteration)\n",
    "\n",
    "                            if iteration%5==0:\n",
    "                                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                                      \"Iteration: {}\".format(iteration),\n",
    "                                      \"Train loss: {:.3f}\".format(loss))\n",
    "\n",
    "                            if iteration%285==0:\n",
    "                                val_acc = []\n",
    "                                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                                for x, y in get_batches(val_x, val_y, batch_size):\n",
    "                                    feed = {inputs_: x,\n",
    "                                            labels_: y[:, None],\n",
    "                                            keep_prob: 1,\n",
    "                                            initial_state: val_state}\n",
    "                                    summary, batch_acc, val_state = sess.run([merged, accuracy, final_state], feed_dict=feed)\n",
    "                                    val_acc.append(batch_acc)\n",
    "                                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "\n",
    "                                #test_writer.add_summary(summary, iteration)\n",
    "\n",
    "                            iteration +=1\n",
    "                    saver.save(sess, \"checkpoints/ownerNameCateg.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_acc = []\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    for ii, (x, y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n",
    "        feed = {inputs_: x,\n",
    "                labels_: y[:, None],\n",
    "                keep_prob: 1,\n",
    "                initial_state: test_state}\n",
    "        batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test accuracy: {:.3f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = \"Dushyant Sekhar \"\n",
    "\n",
    "x_int = [vocab_to_int[word] for word in x.split()]\n",
    "x_int = x_int[:28] #ignore words after 28th\n",
    "\n",
    "x_int_sized = np.zeros((1,seq_len), dtype=int)\n",
    "x_int_sized[0,-len(x_int):] = np.array(x_int)[:seq_len]\n",
    "print(x_int_sized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fillerSize =  batch_size - 1\n",
    "filler = np.tile(np.zeros((1, seq_len), dtype=int),(fillerSize,1))\n",
    "#print(filler.shape)\n",
    "prodBatch = np.append(x_int_sized,filler, axis=0)\n",
    "#print(prodBatch)\n",
    "#print(prodBatch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    feed = {inputs_: prodBatch,\n",
    "            keep_prob: 1,\n",
    "            initial_state: test_state}\n",
    "    output = sess.run(predictions, feed_dict=feed)\n",
    "    print(output[0])\n",
    "    if output[0]>0.5:\n",
    "        print(x,\"\\nCompany\")\n",
    "        print(\"probability {}%\".format(np.round(output[0][0]*100,2)))\n",
    "    else:\n",
    "        print(x,\"\\nPerson\")\n",
    "        print(\"probability {}%\".format(np.round(100-output[0][0]*100,2)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
