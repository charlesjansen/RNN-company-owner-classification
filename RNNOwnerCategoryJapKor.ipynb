{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Owner Categorization with an RNN\n",
    "\n",
    "In this notebook, I will implement a recurrent neural network that categorize owners base on their name. Using an RNN rather than a feedfoward network is more accurate since we can include information about the *sequence* of words. Here we'll use a dataset of owners name from the Philipines and India.\n",
    "\n",
    "The architecture for this network is shown below.\n",
    "\n",
    "Here, we'll pass in words to an embedding layer.\n",
    "\n",
    "From the embedding layer, the new representations will be passed to LSTM cells. These will add recurrent connections to the network so we can include information about the sequence of words in the data. Finally, the LSTM cells will go to a sigmoid output layer here. The output layer will just be a single unit then, with a sigmoid activation function.\n",
    "\n",
    "We are not interested in the sigmoid outputs except for the very last one, we can ignore the rest. We'll calculate the cost from the output of the last step and the training label.\n",
    "\n",
    "Charles Jansen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf #TensorFlow 1.0\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xlrd in c:\\programdata\\anaconda3\\lib\\site-packages\n"
     ]
    }
   ],
   "source": [
    "!pip install xlrd\n",
    "fullExcel = pd.read_excel(\"ownersPope.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 117549 entries, 0 to 117548\n",
      "Data columns (total 4 columns):\n",
      "ownername                     117549 non-null object\n",
      "nonPortfolioHolderTypeName    117549 non-null object\n",
      "Country                       117549 non-null object\n",
      "Unnamed: 3                    117549 non-null int64\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "fullExcel.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ownername</th>\n",
       "      <th>nonPortfolioHolderTypeName</th>\n",
       "      <th>Country</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v</td>\n",
       "      <td>Person</td>\n",
       "      <td>Pope</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Na</td>\n",
       "      <td>Person</td>\n",
       "      <td>Pope</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NPI</td>\n",
       "      <td>Company</td>\n",
       "      <td>Pope</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GBC</td>\n",
       "      <td>Company</td>\n",
       "      <td>Pope</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TIS</td>\n",
       "      <td>Company</td>\n",
       "      <td>Pope</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ownername nonPortfolioHolderTypeName Country  Unnamed: 3\n",
       "0         v                     Person    Pope           1\n",
       "1        Na                     Person    Pope           2\n",
       "2       NPI                    Company    Pope           3\n",
       "3       GBC                    Company    Pope           3\n",
       "4       TIS                    Company    Pope           3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullExcel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232    Sanofi\n",
       "233    UPSIDC\n",
       "234    SPRING\n",
       "235    BIH SA\n",
       "Name: ownername, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullExcel.ownername[232:236]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "names = np.array(fullExcel.ownername)\n",
    "categories =  np.array(fullExcel.nonPortfolioHolderTypeName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['v' 'Na' 'NPI']\n",
      "['Person' 'Person' 'Company']\n"
     ]
    }
   ],
   "source": [
    "print(names[:3])\n",
    "print(categories[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data preprocessing\n",
    "\n",
    "Since we're using embedding layers, we'll need to encode each word with an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "all_names = ' '.join(names)\n",
    "words = all_names.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v Na NPI GBC TIS DIC KYB AGS DTS KEC TDK ZEN IHI KSK TPR KOA NOK TMY SMC JNC IPC JSR NTC ASO SRA HKY JCU NDS AAA DAM MMS NTN UNS NAC IIB HSM CBC DRP YSS BEC MBL HCI NBI SRI ISE Aex AMG NJS JSP TKJ JST MSS TOS HKL CDG AOI EKS GMS MMK THK AMC CIC KWS DEN ICF FJP IHN TCS TMK SHO WWG SRF SNI SMT LIC UTI SMN SUS NOB ido PTB BMS TKO GSK KDM Nil Fcb Anu Ffi Lic Fdi Sbi Uti P.c MYH KDB RAG KTB rwt NH() SM2M WEMS SCSK KDDI EKYT UACJ SBBM TOTO ATRA ACAJ IDEC JUKI UBIC CBNY EIZO MONY LINE HUMO BOZO LS.M Kazy SAFE MARU Rayo Feel GPIF Asha Mala Renu Usha Arti Abcd Fccb Amar SAMA KTB() ()AMC KAERI POSCO MOBIS TORAY Osang M-Tek Xymax MaJin LIXIL Benri KISCO UYEKI ADEKA HAYAT J・ART SAJAP NIPPO T・B・H J・M・T OKURA OKOZE ASADA K I M GAUDI TAMAX E・E・Y S-CAN JJEHD S・TEC SEIKO BOSCH HESTA TIDCO KSIDC MPEDA BANKS OHANA T-SKY i.mco TRIAD LU LI Ha Su K.KIM WU YU Ashok Swati Y.uma J Uma Deepa Apidc Shamo Banks Rahul Shama Mamta Neena Latha Pooja Rekha Kamla Nisha Divya Sonia Saroj Swamy Madan Vidya Sapna Raj S S'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_names[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['v',\n",
       " 'Na',\n",
       " 'NPI',\n",
       " 'GBC',\n",
       " 'TIS',\n",
       " 'DIC',\n",
       " 'KYB',\n",
       " 'AGS',\n",
       " 'DTS',\n",
       " 'KEC',\n",
       " 'TDK',\n",
       " 'ZEN',\n",
       " 'IHI',\n",
       " 'KSK',\n",
       " 'TPR',\n",
       " 'KOA',\n",
       " 'NOK',\n",
       " 'TMY',\n",
       " 'SMC',\n",
       " 'JNC']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Encoding the words\n",
    "\n",
    "The embedding lookup requires that we pass in integers to our network. The easiest way to do this is to create dictionaries that map the words in the vocabulary to integers. Then we can convert each of our owner names into integers so they can be passed into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii-1 for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "names_ints = []\n",
    "for each in names:\n",
    "    names_ints.append([vocab_to_int[word] for word in each.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Encoding the labels (= the category here)\n",
    "\n",
    "Our labels are \"company\" or \"person\". To use these labels in our network, we need to convert them to 0 and 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "categories = np.array([1 if each == 'Company' else 0 for each in categories])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 ..., 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length names: 0\n",
      "Maximum name length: 28\n"
     ]
    }
   ],
   "source": [
    "names_lens = Counter([len(x) for x in names_ints])\n",
    "print(\"Zero-length names: {}\".format(names_lens[0]))\n",
    "print(\"Maximum name length: {}\".format(max(names_lens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For names shorter than 28, we'll pad with 0s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "seq_len = max(names_lens)\n",
    "features = np.zeros((len(names_ints), seq_len), dtype=int)\n",
    "for i, row in enumerate(names_ints):\n",
    "    features[i, -len(row):] = np.array(row)[:seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "        53682],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "          923],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "        48125],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "        39767],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "        50102],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "         6666],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "        20285],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "        11005],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "        18317],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "        13957]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[:10,:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training, Validation, Test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "With our data in nice shape, we'll split it into training, validation, and test sets.\n",
    "\n",
    "10% randomly taken for test.\n",
    "\n",
    "Kfold on the remaining 90% for validation and training.-->canceled. Done, but took much more time for the same 98.9 result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "X\n",
      "Train set: \t\t(95214, 28) \n",
      "Validation set: \t(10580, 28) \n",
      "Test set: \t\t(11755, 28) \n",
      "Y\n",
      "Train set: \t\t(95214,) \n",
      "Validation set: \t(10580,) \n",
      "Test set: \t\t(11755,)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.9\n",
    "train_val_x, test_x, train_val_y, test_y = train_test_split(\n",
    "    features, categories, \n",
    "    train_size = split_frac)\n",
    "\n",
    "#sin Kfold\n",
    "train_x, val_x, train_y, val_y = train_test_split(\n",
    "    train_val_x, train_val_y, \n",
    "    train_size = split_frac)\n",
    "'''\n",
    "#Kfold\n",
    "train_x = []\n",
    "val_x   = []\n",
    "train_y = []\n",
    "val_y   = []\n",
    "train_x =  np.empty([0,max(names_lens)])\n",
    "val_x   =  np.empty([0,max(names_lens)])\n",
    "train_y =  np.empty(0)\n",
    "val_y   =  np.empty(0)\n",
    "\n",
    "kf = KFold(n_splits = 9, shuffle=True)\n",
    "for train_index, val_index in kf.split(train_val_x):\n",
    "    train_temp_x, val_temp_x = train_val_x[train_index], train_val_x[val_index]\n",
    "    train_temp_y, val_temp_y = train_val_y[train_index], train_val_y[val_index]\n",
    "    train_x = np.concatenate((train_x, train_temp_x), axis=0)\n",
    "    val_x   = np.concatenate((val_x, val_temp_x), axis=0)\n",
    "    train_y = np.concatenate((train_y, train_temp_y), axis=0)\n",
    "    val_y = np.concatenate((val_y, val_temp_y), axis=0)\n",
    "\n",
    "'''\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"X\\nTrain set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape),\n",
    "      \"\\nY\\nTrain set: \\t\\t{}\".format(train_y.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_y.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_y.shape),\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the graph\n",
    "\n",
    "Here, we'll build the graph. First up, defining the hyperparameters.\n",
    "\n",
    "* `lstm_size`: Number of units in the hidden layers in the LSTM cells. \n",
    "* `lstm_layers`: Number of LSTM layers in the network. I'd start with 1, then add more if I'm underfitting.\n",
    "* `batch_size`: The number of names to feed the network in one training pass.\n",
    "* `learning_rate`: Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lstm_size = 512\n",
    "lstm_layers = 3\n",
    "batch_size = 500\n",
    "learning_rate = 0.003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For the network itself, we'll be passing in our 28 element long names vectors. Each batch will be `batch_size` vectors. We'll also be using dropout on the LSTM layer, so we'll make a placeholder for the keep probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_words = len(vocab_to_int)\n",
    "\n",
    "# Create the graph object\n",
    "graph = tf.Graph()\n",
    "# Add nodes to the graph\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    labels_ = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Embedding\n",
    "\n",
    "Now we'll add an embedding layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56699"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LIMITED',\n",
       " 'Ltd.',\n",
       " 'Co.,',\n",
       " 'LTD',\n",
       " 'PRIVATE',\n",
       " 'Kim',\n",
       " 'Limited',\n",
       " 'Lee',\n",
       " 'Kumar',\n",
       " 'Shah',\n",
       " 'PVT',\n",
       " 'Ltd',\n",
       " 'Association',\n",
       " 'LTD.',\n",
       " '&',\n",
       " 'Private',\n",
       " 'Park',\n",
       " 'Patel',\n",
       " 'Jain',\n",
       " 'Corporation']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Size of the embedding vectors (number of units in the embedding layer)\n",
    "embed_size = 300 \n",
    "\n",
    "with graph.as_default():\n",
    "    embedding = tf.Variable(tf.truncated_normal((n_words, embed_size), stddev=0.1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### LSTM cell\n",
    "\n",
    "\n",
    "\n",
    "Next, we'll create our LSTM cells to use in the recurrent network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    # Your basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # Add dropout to the cell\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n",
    "    \n",
    "    # Getting an initial state of all zeros\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### RNN forward pass\n",
    "\n",
    "\n",
    "Now we need to actually run the data through the RNN nodes. \n",
    "\n",
    "Above I created an initial state, `initial_state`, to pass to the RNN. This is the cell state that is passed between the hidden layers in successive time steps. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, embed,\n",
    "                                             initial_state=initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output\n",
    "\n",
    "We want the final output. So we need to grab the last output with `outputs[:, -1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, activation_fn=tf.sigmoid)\n",
    "    cost = tf.losses.mean_squared_error(labels_, predictions)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Validation accuracy\n",
    "\n",
    "Here we can add a few nodes to calculate the accuracy which we'll use in the validation pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Batching\n",
    "\n",
    "This is a simple function for returning batches from our data. First it removes data such that we only have full batches. Then it iterates through the `x` and `y` arrays and returns slices out of those arrays with size `[batch_size]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size=100):\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "import os\n",
    "epochs = 1\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        for ii, (x, y) in enumerate(get_batches(train_x, train_y, 2), 1):\n",
    "            print(ii)\n",
    "            print(x)\n",
    "            print(y)\n",
    "            print(x.shape)\n",
    "            print(y.shape)\n",
    "            print(y[:, None])\n",
    "            if ii == 5:\n",
    "                raise Exception(\"Manual Stop\")\n",
    "#''' ;           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/3 Iteration: 5 Train loss: 0.325\n",
      "Epoch: 0/3 Iteration: 10 Train loss: 0.255\n",
      "Epoch: 0/3 Iteration: 15 Train loss: 0.224\n",
      "Epoch: 0/3 Iteration: 20 Train loss: 0.216\n",
      "Epoch: 0/3 Iteration: 25 Train loss: 0.179\n",
      "Epoch: 0/3 Iteration: 30 Train loss: 0.194\n",
      "Epoch: 0/3 Iteration: 35 Train loss: 0.189\n",
      "Epoch: 0/3 Iteration: 40 Train loss: 0.170\n",
      "Epoch: 0/3 Iteration: 45 Train loss: 0.144\n",
      "Epoch: 0/3 Iteration: 50 Train loss: 0.157\n",
      "Epoch: 0/3 Iteration: 55 Train loss: 0.121\n",
      "Epoch: 0/3 Iteration: 60 Train loss: 0.099\n",
      "Epoch: 0/3 Iteration: 65 Train loss: 0.043\n",
      "Epoch: 0/3 Iteration: 70 Train loss: 0.035\n",
      "Epoch: 0/3 Iteration: 75 Train loss: 0.028\n",
      "Epoch: 0/3 Iteration: 80 Train loss: 0.024\n",
      "Epoch: 0/3 Iteration: 85 Train loss: 0.026\n",
      "Epoch: 0/3 Iteration: 90 Train loss: 0.014\n",
      "Epoch: 0/3 Iteration: 95 Train loss: 0.012\n",
      "Epoch: 0/3 Iteration: 100 Train loss: 0.018\n",
      "Epoch: 0/3 Iteration: 105 Train loss: 0.020\n",
      "Epoch: 0/3 Iteration: 110 Train loss: 0.021\n",
      "Epoch: 0/3 Iteration: 115 Train loss: 0.021\n",
      "Epoch: 0/3 Iteration: 120 Train loss: 0.007\n",
      "Epoch: 0/3 Iteration: 125 Train loss: 0.013\n",
      "Epoch: 0/3 Iteration: 130 Train loss: 0.023\n",
      "Epoch: 0/3 Iteration: 135 Train loss: 0.008\n",
      "Epoch: 0/3 Iteration: 140 Train loss: 0.024\n",
      "Epoch: 0/3 Iteration: 145 Train loss: 0.020\n",
      "Epoch: 0/3 Iteration: 150 Train loss: 0.018\n",
      "Epoch: 0/3 Iteration: 155 Train loss: 0.017\n",
      "Epoch: 0/3 Iteration: 160 Train loss: 0.019\n",
      "Epoch: 0/3 Iteration: 165 Train loss: 0.015\n",
      "Epoch: 0/3 Iteration: 170 Train loss: 0.016\n",
      "Epoch: 0/3 Iteration: 175 Train loss: 0.008\n",
      "Epoch: 0/3 Iteration: 180 Train loss: 0.020\n",
      "Epoch: 0/3 Iteration: 185 Train loss: 0.015\n",
      "Epoch: 0/3 Iteration: 190 Train loss: 0.015\n",
      "Epoch: 1/3 Iteration: 195 Train loss: 0.004\n",
      "Epoch: 1/3 Iteration: 200 Train loss: 0.019\n",
      "Epoch: 1/3 Iteration: 205 Train loss: 0.008\n",
      "Epoch: 1/3 Iteration: 210 Train loss: 0.018\n",
      "Epoch: 1/3 Iteration: 215 Train loss: 0.017\n",
      "Epoch: 1/3 Iteration: 220 Train loss: 0.010\n",
      "Epoch: 1/3 Iteration: 225 Train loss: 0.020\n",
      "Epoch: 1/3 Iteration: 230 Train loss: 0.016\n",
      "Epoch: 1/3 Iteration: 235 Train loss: 0.010\n",
      "Epoch: 1/3 Iteration: 240 Train loss: 0.008\n",
      "Epoch: 1/3 Iteration: 245 Train loss: 0.012\n",
      "Epoch: 1/3 Iteration: 250 Train loss: 0.003\n",
      "Val acc: 0.983\n",
      "Epoch: 1/3 Iteration: 255 Train loss: 0.009\n",
      "Epoch: 1/3 Iteration: 260 Train loss: 0.003\n",
      "Epoch: 1/3 Iteration: 265 Train loss: 0.005\n",
      "Epoch: 1/3 Iteration: 270 Train loss: 0.007\n",
      "Epoch: 1/3 Iteration: 275 Train loss: 0.011\n",
      "Epoch: 1/3 Iteration: 280 Train loss: 0.003\n",
      "Epoch: 1/3 Iteration: 285 Train loss: 0.004\n",
      "Epoch: 1/3 Iteration: 290 Train loss: 0.009\n",
      "Epoch: 1/3 Iteration: 295 Train loss: 0.007\n",
      "Epoch: 1/3 Iteration: 300 Train loss: 0.005\n",
      "Epoch: 1/3 Iteration: 305 Train loss: 0.013\n",
      "Epoch: 1/3 Iteration: 310 Train loss: 0.004\n",
      "Epoch: 1/3 Iteration: 315 Train loss: 0.005\n",
      "Epoch: 1/3 Iteration: 320 Train loss: 0.011\n",
      "Epoch: 1/3 Iteration: 325 Train loss: 0.003\n",
      "Epoch: 1/3 Iteration: 330 Train loss: 0.008\n",
      "Epoch: 1/3 Iteration: 335 Train loss: 0.007\n",
      "Epoch: 1/3 Iteration: 340 Train loss: 0.008\n",
      "Epoch: 1/3 Iteration: 345 Train loss: 0.008\n",
      "Epoch: 1/3 Iteration: 350 Train loss: 0.007\n",
      "Epoch: 1/3 Iteration: 355 Train loss: 0.008\n",
      "Epoch: 1/3 Iteration: 360 Train loss: 0.004\n",
      "Epoch: 1/3 Iteration: 365 Train loss: 0.002\n",
      "Epoch: 1/3 Iteration: 370 Train loss: 0.010\n",
      "Epoch: 1/3 Iteration: 375 Train loss: 0.005\n",
      "Epoch: 1/3 Iteration: 380 Train loss: 0.001\n",
      "Epoch: 2/3 Iteration: 385 Train loss: 0.002\n",
      "Epoch: 2/3 Iteration: 390 Train loss: 0.003\n",
      "Epoch: 2/3 Iteration: 395 Train loss: 0.003\n",
      "Epoch: 2/3 Iteration: 400 Train loss: 0.007\n",
      "Epoch: 2/3 Iteration: 405 Train loss: 0.005\n",
      "Epoch: 2/3 Iteration: 410 Train loss: 0.002\n",
      "Epoch: 2/3 Iteration: 415 Train loss: 0.006\n",
      "Epoch: 2/3 Iteration: 420 Train loss: 0.006\n",
      "Epoch: 2/3 Iteration: 425 Train loss: 0.003\n",
      "Epoch: 2/3 Iteration: 430 Train loss: 0.003\n",
      "Epoch: 2/3 Iteration: 435 Train loss: 0.004\n",
      "Epoch: 2/3 Iteration: 440 Train loss: 0.000\n",
      "Epoch: 2/3 Iteration: 445 Train loss: 0.006\n",
      "Epoch: 2/3 Iteration: 450 Train loss: 0.004\n",
      "Epoch: 2/3 Iteration: 455 Train loss: 0.001\n",
      "Epoch: 2/3 Iteration: 460 Train loss: 0.004\n",
      "Epoch: 2/3 Iteration: 465 Train loss: 0.010\n",
      "Epoch: 2/3 Iteration: 470 Train loss: 0.002\n",
      "Epoch: 2/3 Iteration: 475 Train loss: 0.004\n",
      "Epoch: 2/3 Iteration: 480 Train loss: 0.006\n",
      "Epoch: 2/3 Iteration: 485 Train loss: 0.000\n",
      "Epoch: 2/3 Iteration: 490 Train loss: 0.002\n",
      "Epoch: 2/3 Iteration: 495 Train loss: 0.006\n",
      "Epoch: 2/3 Iteration: 500 Train loss: 0.000\n",
      "Val acc: 0.987\n",
      "Epoch: 2/3 Iteration: 505 Train loss: 0.003\n",
      "Epoch: 2/3 Iteration: 510 Train loss: 0.008\n",
      "Epoch: 2/3 Iteration: 515 Train loss: 0.005\n",
      "Epoch: 2/3 Iteration: 520 Train loss: 0.000\n",
      "Epoch: 2/3 Iteration: 525 Train loss: 0.004\n",
      "Epoch: 2/3 Iteration: 530 Train loss: 0.006\n",
      "Epoch: 2/3 Iteration: 535 Train loss: 0.006\n",
      "Epoch: 2/3 Iteration: 540 Train loss: 0.006\n",
      "Epoch: 2/3 Iteration: 545 Train loss: 0.006\n",
      "Epoch: 2/3 Iteration: 550 Train loss: 0.000\n",
      "Epoch: 2/3 Iteration: 555 Train loss: 0.004\n",
      "Epoch: 2/3 Iteration: 560 Train loss: 0.006\n",
      "Epoch: 2/3 Iteration: 565 Train loss: 0.004\n",
      "Epoch: 2/3 Iteration: 570 Train loss: 0.000\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "            feed = {inputs_: x,\n",
    "                    labels_: y[:, None],\n",
    "                    keep_prob: 0.5,\n",
    "                    initial_state: state}\n",
    "            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)\n",
    "            \n",
    "            if iteration%5==0:\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {:.3f}\".format(loss))\n",
    "\n",
    "            if iteration%250==0:\n",
    "                val_acc = []\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                for x, y in get_batches(val_x, val_y, batch_size):\n",
    "                    feed = {inputs_: x,\n",
    "                            labels_: y[:, None],\n",
    "                            keep_prob: 1,\n",
    "                            initial_state: val_state}\n",
    "                    batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                    val_acc.append(batch_acc)\n",
    "                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "            iteration +=1\n",
    "    saver.save(sess, \"checkpoints/ownerNameCategJapKor.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.986\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    for ii, (x, y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n",
    "        feed = {inputs_: x,\n",
    "                labels_: y[:, None],\n",
    "                keep_prob: 1,\n",
    "                initial_state: test_state}\n",
    "        batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test accuracy: {:.3f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0 5087 2461]]\n"
     ]
    }
   ],
   "source": [
    "x = \"Dushyant Sekhar \"\n",
    "\n",
    "x_int = [vocab_to_int[word] for word in x.split()]\n",
    "x_int = x_int[:28] #ignore words after 28th\n",
    "\n",
    "x_int_sized = np.zeros((1,seq_len), dtype=int)\n",
    "x_int_sized[0,-len(x_int):] = np.array(x_int)[:seq_len]\n",
    "print(x_int_sized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fillerSize =  batch_size - 1\n",
    "filler = np.tile(np.zeros((1, seq_len), dtype=int),(fillerSize,1))\n",
    "#print(filler.shape)\n",
    "prodBatch = np.append(x_int_sized,filler, axis=0)\n",
    "#print(prodBatch)\n",
    "#print(prodBatch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00273275]\n",
      "Dushyant Sekhar  \n",
      "Person\n",
      "probability 99.73%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    feed = {inputs_: prodBatch,\n",
    "            keep_prob: 1,\n",
    "            initial_state: test_state}\n",
    "    output = sess.run(predictions, feed_dict=feed)\n",
    "    print(output[0])\n",
    "    if output[0]>0.5:\n",
    "        print(x,\"\\nCompany\")\n",
    "        print(\"probability {}%\".format(np.round(output[0][0]*100,2)))\n",
    "    else:\n",
    "        print(x,\"\\nPerson\")\n",
    "        print(\"probability {}%\".format(np.round(100-output[0][0]*100,2)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
